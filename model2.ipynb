{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "described-elizabeth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 101348.88 1]\n",
      " [502 'France' 'Female' ... 0 113931.57 1]\n",
      " [645 'Spain' 'Male' ... 0 149756.71 1]\n",
      " ...\n",
      " [597 'France' 'Female' ... 0 69384.71 1]\n",
      " [709 'France' 'Female' ... 1 42085.58 1]\n",
      " [772 'Germany' 'Male' ... 0 92888.52 1]]\n",
      "['High Service Charges/Rate of Interest' 'Long Response Times'\n",
      " 'High Service Charges/Rate of Interest' ... 'Long Response Times'\n",
      " 'Inexperienced Staff / Bad customer service ' 'Excess Documents Required']\n",
      "test here [[502 'France' 'Female' 49 7 12966.82 3 1 0 103031.57 0.7198832]\n",
      " [710 'France' 'Female' 29 1 0.0 3 0 0 83826.63 0.5143332]\n",
      " [482 'Spain' 'Male' 42 2 10681.53 1 1 0 128643.35 0.50090337]\n",
      " [511 'Spain' 'Female' 27 3 14010.01 3 0 1 87132.2 0.3535122]\n",
      " [450 'Spain' 'Male' 38 1 0.0 1 1 1 70137.92 0.18641391]\n",
      " [728 'France' 'Male' 34 3 0.0 1 0 0 65124.8 0.120752424]\n",
      " [620 'Germany' 'Male' 22 2 0.0 2 1 0 98672.9 0.04301247]\n",
      " [661 'France' 'Female' 33 6 0.0 2 0 1 39922.7 0.022441506]]\n",
      "[0.7198832  0.5143332  0.50090337 0.3535122  0.18641391 0.12075242\n",
      " 0.04301247 0.02244151]\n",
      "(8, 11)\n",
      "(2037, 11)\n",
      "Epoch 1/50\n",
      "204/204 [==============================] - 2s 2ms/step - loss: 0.7043\n",
      "Epoch 2/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.6200\n",
      "Epoch 3/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5769\n",
      "Epoch 4/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5676\n",
      "Epoch 5/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5646\n",
      "Epoch 6/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5647\n",
      "Epoch 7/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5633\n",
      "Epoch 8/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5618\n",
      "Epoch 9/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5611\n",
      "Epoch 10/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5623\n",
      "Epoch 11/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5617\n",
      "Epoch 12/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5598\n",
      "Epoch 13/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5598\n",
      "Epoch 14/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5575\n",
      "Epoch 15/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5599\n",
      "Epoch 16/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5591\n",
      "Epoch 17/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5594\n",
      "Epoch 18/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5584\n",
      "Epoch 19/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5573\n",
      "Epoch 20/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5571\n",
      "Epoch 21/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5580\n",
      "Epoch 22/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5575\n",
      "Epoch 23/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5577\n",
      "Epoch 24/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5588\n",
      "Epoch 25/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5575\n",
      "Epoch 26/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5543\n",
      "Epoch 27/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5562\n",
      "Epoch 28/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5561\n",
      "Epoch 29/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5567\n",
      "Epoch 30/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5571\n",
      "Epoch 31/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5555\n",
      "Epoch 32/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5556\n",
      "Epoch 33/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5563\n",
      "Epoch 34/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5554\n",
      "Epoch 35/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5546\n",
      "Epoch 36/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5570\n",
      "Epoch 37/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5582\n",
      "Epoch 38/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5545\n",
      "Epoch 39/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5536\n",
      "Epoch 40/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5536\n",
      "Epoch 41/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5549\n",
      "Epoch 42/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5545\n",
      "Epoch 43/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5566\n",
      "Epoch 44/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5551\n",
      "Epoch 45/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5549\n",
      "Epoch 46/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5553\n",
      "Epoch 47/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5533\n",
      "Epoch 48/50\n",
      "204/204 [==============================] - 0s 2ms/step - loss: 0.5555\n",
      "Epoch 49/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5544\n",
      "Epoch 50/50\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.5520\n",
      "[[0.2690161  0.24435163 0.2457987  0.26948127]\n",
      " [0.25756696 0.3362586  0.0780566  0.30523485]\n",
      " [0.28251845 0.25815856 0.2061128  0.2700767 ]\n",
      " [0.1813252  0.35779166 0.20339534 0.25596267]\n",
      " [0.2247456  0.3032707  0.24687192 0.26664814]\n",
      " [0.27453113 0.24897856 0.17716748 0.263188  ]\n",
      " [0.22677493 0.29264843 0.18662485 0.2639095 ]\n",
      " [0.14448074 0.39439243 0.22733936 0.23737672]]\n",
      "=================================================================================================\n",
      "the Excess Documents Required  % 26.90160870552063\n",
      "the High Service Charges/Rate of Interest  % 24.43516254425049\n",
      "the Inexperienced Staff / Bad customer service   % 24.57987070083618\n",
      "the Long Response Times  % 26.948127150535583\n",
      "=================================================================================================\n",
      "the Excess Documents Required  % 25.756695866584778\n",
      "the High Service Charges/Rate of Interest  % 33.6258590221405\n",
      "the Inexperienced Staff / Bad customer service   % 7.805660367012024\n",
      "the Long Response Times  % 30.52348494529724\n",
      "=================================================================================================\n",
      "the Excess Documents Required  % 28.25184464454651\n",
      "the High Service Charges/Rate of Interest  % 25.815856456756592\n",
      "the Inexperienced Staff / Bad customer service   % 20.6112802028656\n",
      "the Long Response Times  % 27.00766921043396\n",
      "=================================================================================================\n",
      "the Excess Documents Required  % 18.132519721984863\n",
      "the High Service Charges/Rate of Interest  % 35.77916622161865\n",
      "the Inexperienced Staff / Bad customer service   % 20.33953368663788\n",
      "the Long Response Times  % 25.596266984939575\n",
      "=================================================================================================\n",
      "the Excess Documents Required  % 22.474560141563416\n",
      "the High Service Charges/Rate of Interest  % 30.327069759368896\n",
      "the Inexperienced Staff / Bad customer service   % 24.68719184398651\n",
      "the Long Response Times  % 26.664814352989197\n",
      "=================================================================================================\n",
      "the Excess Documents Required  % 27.453112602233887\n",
      "the High Service Charges/Rate of Interest  % 24.897855520248413\n",
      "the Inexperienced Staff / Bad customer service   % 17.716747522354126\n",
      "the Long Response Times  % 26.318800449371338\n",
      "=================================================================================================\n",
      "the Excess Documents Required  % 22.67749309539795\n",
      "the High Service Charges/Rate of Interest  % 29.264843463897705\n",
      "the Inexperienced Staff / Bad customer service   % 18.662485480308533\n",
      "the Long Response Times  % 26.39094889163971\n",
      "=================================================================================================\n",
      "the Excess Documents Required  % 14.448073506355286\n",
      "the High Service Charges/Rate of Interest  % 39.43924307823181\n",
      "the Inexperienced Staff / Bad customer service   % 22.733935713768005\n",
      "the Long Response Times  % 23.737671971321106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "dataset = pd.read_csv('data_re.csv')\n",
    "#test = pd.read_csv('testtest1.csv')\n",
    "#data_re=dataset[dataset['Exited']==1]\n",
    "#print(data_re)\n",
    "#data_re.to_csv('data_re.csv')\n",
    "#oth=dict(\"Reason for exiting company\"=[])\n",
    "#test=test[test['Exited']==1]\n",
    "#test.set_index('RowNumber',inplace=True)\n",
    "#test.insert(loc=13,column=\"Reason for exiting company\",value=None,inplace=True)\n",
    "#test.add(oth,axis=\"columns\", fill_value=None)\n",
    "# print(len(test.iloc[0]))\n",
    "#test.to_csv('ttre.csv')\n",
    "test=pd.read_csv('testtest1.csv')\n",
    "\n",
    "X = dataset.iloc[:, 3:14].values\n",
    "X_test=test.iloc[:, 3:14].values\n",
    "\n",
    "y= dataset.iloc[:, 14].values\n",
    "y_test= test.iloc[:, 13].values\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "print(\"test here\",X_test)\n",
    "print(y_test)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "\n",
    "#onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "#X = onehotencoder.fit_transform(X).toarray()\n",
    "#X = X[:, 1:]\n",
    "# labelencoder_X_re = LabelEncoder()#creating label encoder object no. 1 to encode region name(index 1 in features)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# print(\"before\",y)\n",
    "y = y.reshape(len(y), 1)\n",
    "# print(\"abefore\",y)\n",
    "y= onehot_encoder.fit_transform(y)\n",
    "# print(y)\n",
    "# pp\n",
    "labelencoder_X_3 = LabelEncoder()#creating label encoder object no. 1 to encode region name(index 1 in features)\n",
    "X_test[:, 1] = labelencoder_X_3.fit_transform(X_test[:, 1])#encoding region from string to just 3 no.s 0,1,2 respectively\n",
    "labelencoder_X_4 = LabelEncoder()\n",
    "X_test[:, 2] = labelencoder_X_4.fit_transform(X_test[:, 2])#encoding Gender from string to just 2 no.s 0,1(male,female) respectively\n",
    "\n",
    "#onehotencoder2 = OneHotEncoder(categorical_features = [1])\n",
    "#X_test= onehotencoder2.fit_transform(X_test).toarray()\n",
    "#X_test = X_test[:, 1:]\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "X_train=X\n",
    "y_train=y\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.models import Sequential#For building the Neural Network layer by layer\n",
    "from keras.layers import Dense\n",
    "#------2)Defining a Graph\n",
    "classifier = Sequential() #UNCOMMENT if not running from saved model\n",
    "classifier.add(Dense(units = 6, kernel_initializer='glorot_uniform', activation = 'relu', input_dim = 11))#UNCOMMENT if not running from saved model\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer='glorot_uniform', activation = 'relu'))#UNCOMMENT if not running from saved model\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 4, kernel_initializer='glorot_uniform', activation = 'sigmoid'))#UNCOMMENT if not running from saved model\n",
    "\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy')#UNCOMMENT if not running from saved model\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train,batch_size=10,epochs=50)#UNCOMMENT if not running from saved model\n",
    "classifier.save('my_model2.h5')  # creates a HDF5 file 'my_model.h5'#UNCOMMENT if not running from saved model\n",
    "#classifier=load_model('my_model2.h5') #UNCOMMENT if running from the saved model\n",
    "y_pred = classifier.predict(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "print(y_pred)\n",
    "# dff=pd.read_csv(\"testtest.csv\")\n",
    "# dff['Exited']=y_pred\n",
    "# dff.set_index('RowNumber',inplace=True)\n",
    "# dff.sort_values('Exited',ascending=False,inplace=True)\n",
    "# dff.to_csv('testtest1.csv') #output file\n",
    "\n",
    "for j in range(len(y_pred)):\n",
    "    print(\"=================================================================================================\")\n",
    "    for (label, p) in zip(label_encoder.classes_, y_pred[j]):\n",
    "        print(\"the\",label,\" %\", p*100 )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
